<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Finite-sample analysis of $M$-estimators using self-concordance, Part I | Personal blog of Dmitrii Ostrovskii</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Finite-sample analysis of $M$-estimators using self-concordance, Part I" />
<meta name="author" content="Dmitrii M. Ostrovskii" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this series of posts, I will present our recent work with Francis Bach on the optimal rates for $M$-estimators with self-concordant-like losses. The term “$M$-estimator” is more commonly used in the statistical community; in the learning theory community one more often talks about empirical risk minimization. The two are the same; I will mostly use the statistical terminology to stress the connections to the classical asymptotic results." />
<meta property="og:description" content="In this series of posts, I will present our recent work with Francis Bach on the optimal rates for $M$-estimators with self-concordant-like losses. The term “$M$-estimator” is more commonly used in the statistical community; in the learning theory community one more often talks about empirical risk minimization. The two are the same; I will mostly use the statistical terminology to stress the connections to the classical asymptotic results." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/11/12/self-concordance-part-1.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/11/12/self-concordance-part-1.html" />
<meta property="og:site_name" content="Personal blog of Dmitrii Ostrovskii" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-11-12T00:00:00+01:00" />
<script type="application/ld+json">
{"description":"In this series of posts, I will present our recent work with Francis Bach on the optimal rates for $M$-estimators with self-concordant-like losses. The term “$M$-estimator” is more commonly used in the statistical community; in the learning theory community one more often talks about empirical risk minimization. The two are the same; I will mostly use the statistical terminology to stress the connections to the classical asymptotic results.","author":{"@type":"Person","name":"Dmitrii M. Ostrovskii"},"@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/11/12/self-concordance-part-1.html","headline":"Finite-sample analysis of $M$-estimators using self-concordance, Part I","dateModified":"2018-11-12T00:00:00+01:00","datePublished":"2018-11-12T00:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/11/12/self-concordance-part-1.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Personal blog of Dmitrii Ostrovskii" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Personal blog of Dmitrii Ostrovskii</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Finite-sample analysis of $M$-estimators using self-concordance, Part I</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-11-12T00:00:00+01:00" itemprop="datePublished">Nov 12, 2018
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Dmitrii M. Ostrovskii</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In this series of posts, I will present our <a href="https://arxiv.org/abs/1810.06838"><strong>recent work with Francis Bach</strong></a> on the optimal rates for $M$-estimators with self-concordant-like losses. The term “$M$-estimator” is more commonly used in the statistical community; in the learning theory community one more often talks about empirical risk minimization. 
The two are the same; I will mostly use the statistical terminology to stress the connections to the classical asymptotic results.</p>

<h1 id="setting"><strong>Setting</strong></h1>

<p>We work in the familiar statistical learning setting: the goal is to predict the <em>label</em> $Y \in \mathcal{Y}$ as some function of the dot product $\eta = X^\top \theta$ between the <em>predictor</em> $X \in \mathbb{R}^d$ and <em>parameter</em> $\theta \in \mathbb{R}^d$. This is formalized as minimization of <strong>expected risk</strong>
\[
L(\theta) := \mathbb{E}[\ell(Y,X^\top\theta)],
\]
where $\ell: \mathcal{Y} \times \mathbb{R} \to \mathbb{R}$ is some <em>loss function</em>, and $\mathbb{E}[\cdot]$ is expectation over the distribution $\mathcal{P}$ of the observation $Z = (X,Y)$. Let $\theta_*$ be a minimizer of $L(\theta)$, which will prove to be unique in our situation.
Since the distribution $\mathcal{P}$ is unknown, we cannot find $\theta_*$ directly. Instead we observe an i.i.d. sample 
\[
(X_1,Y_1), …, (X_n, Y_n)
\] 
from $\mathcal{P}$, and compute the empirical counterpart of $\theta_*$, a minimizer $\widehat \theta_n$ of <strong>empirical risk</strong>
\[
L_n(\theta) := \frac{1}{n}\sum_{i=1}^n \ell(Y_i,X_i^\top\theta).
\]
This setting includes regression in the case $\mathcal{Y} = \mathbb{R}$ (usually one also takes $\ell(y,y’) = \varphi(y-y’)$ for some function $\varphi: \mathbb{R} \to \mathbb{R}$, e.g., the squared loss) and classification in the case $\mathcal{Y} = \{0,1\}$.</p>

<p>To understand our somewhat unusual loss parametrization, note that when 
$
\ell(y,\eta) = - \log p_{\eta}(y)
$ 
for some probability distribution $p_{\eta}(\cdot)$ on $\mathcal{Y}$ parametrized by $\eta \in \mathbb{R}$, we have a conditional likelihood model for $Y$ given $X^{\top} \theta$, and then $\widehat \theta_n$ becomes the maximum likelihood estimator (MLE). This includes, in particular, <em>least squares</em> and <em>logistic regression</em>. More generally, this is the case in <em>generalized linear models</em> (GLMs) with canonical parametrization, where the loss takes the form
\begin{equation}
\label{def:glm}
\ell(y,\eta) = - y\eta + a(\eta) - b(y),
\end{equation}
where $a(\eta): \mathbb{R} \to \mathbb{R}$, called <em>the cumulant</em>, normalizes $\ell(y,\eta)$ to be a valid negative log-likelihood:
\[
a(\eta) = \log \int_{\mathcal{Y}} \exp(y\eta + b(y)) \, \text{d}y.
\]</p>

<p>More generally, we can talk about <em>quasi-MLE</em> where one removes the implicit assumption that $\mathcal{Y}$ is indeed generated by one of the distributions $p_{\eta}(\cdot)$ for some value $\eta_*$.  In this case, the asymptotic properties of $\widehat\theta_n$ have long been studied in statistics in the theory of <em>Local Asymptotic Normality</em>. Our goal in this work was to extend this theory the finite-sample setting, and beyond the quasi-MLE setup, that is, for general $M$-estimators. We will now make a brief tour of this theory.</p>

<h1 id="overview-of-asymptotic-theory"><strong>Overview of asymptotic theory</strong></h1>
<p>LAN theory concerns the limit $n \to \infty$ with fixed $d$, and relies on the local regularity assumptions, requiring that $L(\theta)$ is sufficiently smooth at $\theta_*$, so that $\nabla L(\theta_*) = 0$, and <em>the excess risk</em>
$
L(\theta) - L(\theta_*)
$
can be well-approximated by its 2nd-order Taylor expansion, or the squared <em>prediction distance</em> 
\[
\frac{1}{2} \Vert \theta - \theta_* \Vert_{\mathbf{H}}^2 := \frac{1}{2} \Vert \mathbf{H}^{1/2}(\theta - \theta_*) \Vert^2,
\]
where $\mathbf{H}$ is the risk Hessian at the optimum:
\[
\mathbf{H} := \nabla^2 L(\theta_*),
\]
and one usually assumes that $\mathbf{H} \succ 0$.
Note that in the case of least squares, we simply have 
\[
L(\theta) - L(\theta_*) = \frac{1}{2} \Vert\theta - \theta_*\Vert_{\mathbf{H}}.
\]
These assumptions allow to derive the <em>local asymptotic normality</em> of quasi MLE, or Fisher’s theorem:
\begin{equation}
\label{eq:lan-fisher}
\sqrt{n}\mathbf{H}^{1/2}(\widehat\theta_n - \theta_*)  \rightsquigarrow \mathcal{N}(0, \mathbf{H}^{-1/2} \mathbf{G} \mathbf{H}^{-1/2}),
\tag{1}
\end{equation}
where $\rightsquigarrow$ is convergence in the law, and $\mathbf{G}$ is the covariance matrix of the loss gradient at $\theta_*$:
\[
\mathbf{G} := \mathbb{E} \left[ \nabla_\theta \ell(Y,X^\top\theta_*) \otimes \nabla_\theta \ell(Y,X^\top\theta_*) \right].
\] 
In well-specified MLE, one has the <em>Bartlett identity</em> $\mathbf{G} = \mathbf{H}$; then, $\mathbf{H}^{-1/2} \mathbf{G} \mathbf{H}^{-1/2}$ is the identity, and 
\[
\text{Tr}[\mathbf{H}^{-1/2} \mathbf{G} \mathbf{H}^{-1/2}] = d.
\]
In the general case, we can define the <em>effective dimension</em>
\[
d_{eff} := \text{Tr} [\mathbf{H}^{-1/2} \mathbf{G} \mathbf{H}^{-1/2}],
\]
and hope that it is not much larger than $d$, i.e., the model is only “moderately” misspecified. 
In some cases this is known to be true: for example, in least-squares regression one has
$
d_{eff} \le \kappa_X \cdot \kappa_Y \cdot d,
$
irrespectively of the true distribution of the noise, whenever $X$ and $Y$ have bounded kurtoses $\kappa_X, \kappa_Y$:
\[
\mathbb{E}[(Y-\mathbb{E}[Y])^4]^{1/4} \le \kappa_Y \cdot \mathbb{E}[(Y-\mathbb{E}[Y])^2]^{1/2},
\]
\[
\mathbb{E}[\left\langle u, X-\mathbb{E}[X] \right\rangle^4]^{1/4} \le \kappa_X \cdot \mathbb{E}[\left\langle u, X-\mathbb{E}[X] \right\rangle^2]^{1/2}, \quad \forall u \in \mathbb{R}^d.
\]</p>

<p>Returning to the results of the LAN theory, \eqref{eq:lan-fisher} implies that the scaled prediction error $n\Vert \widehat\theta_n-\theta_*\Vert_{\mathbf{H}}^2$ asymptotically has the generalized chi-square distribution – namely, it distributed as the square of $\mathcal{N}(0, \mathbf{H}^{-1/2} \mathbf{G} \mathbf{H}^{-1/2})$. 
Moreover, it can also be obtained that $2n[L(\widehat\theta_n) - L(\theta_*)]$ has the same asymptotic distribution. 
Using the standard chi-square deviation bound from <a href="https://projecteuclid.org/euclid.aos/1015957395#info">[1]</a>, we can summarize this result in terms of asymptotic confidence bounds for the excess risk and prediction distance:
\begin{equation}
\label{eq:crb-prob}
\boxed{
\begin{aligned}
L(\widehat\theta_n)  - L(\theta_*) 
&amp;= \frac{1}{2} \Vert\theta - \theta_*\Vert_{\mathbf{H}}^2 + o(n^{-1}) = \frac{d_{eff} (1 + \sqrt{2\log(1/\delta)})^2}{2n} + o(n^{-1}).
\end{aligned}
}
\tag{$\star$}
\end{equation}</p>

<p>In fact, the analysis leading to \eqref{eq:crb-prob} can be summarized in three steps:</p>

<ol>
  <li>
    <p>First, one can easily control the squared “natural” norm of the score, $\Vert\nabla L_n(\theta_*)\Vert_{\mathbf{H}^{-1}}^2$, by the central limit theorem, since it is the average of i.i.d. quantities – squared norms of the gradients
\[
\Vert \nabla_{\theta} \ell(Y_i,X_i^\top \theta_*) \Vert_{\mathbf{H}}^2, \quad 1 \le i \le n.
\]</p>
  </li>
  <li>
    <p>Then one can prove that as $n \to \infty$, the empirical risk can be approximated by its 2nd-order Taylor expansion
\[
L_n(\theta_*) + \left\langle \nabla L_n(\theta_*),\theta - \theta_*\right\rangle + \frac{1}{2} \Vert \widehat\theta_n - \theta_* \Vert^2_{\mathbf{H}_n}
\]
where
\[
\mathbf{H}_n = \nabla^2 L_n(\theta_*)
\]
is the empirical Hessian at $\theta_*$. Since $\mathbf{H}_n$ converges to $\mathbf{H}$ in the positive-semidefinite sense, this allows to <em>localize</em> the estimate:
\begin{align}
0 \ge L_n(\widehat\theta_n) - L_n(\theta_*) 
&amp;\approx \left\langle \nabla L_n(\theta_*),\widehat \theta_n - \theta_*\right\rangle + \frac{1}{2} \Vert\widehat\theta_n - \theta_*\Vert_{\mathbf{H}_n}^2 \\
&amp;\approx \left\langle \nabla L_n(\theta_*),\widehat \theta_n - \theta_*\right\rangle + \frac{1}{2} \Vert\widehat\theta_n - \theta_*\Vert_{\mathbf{H}}^2  \label{eq:localization-chain}\tag{2}\\
&amp;= \left\langle \mathbf{H}^{-1/2} \nabla L_n(\theta_*), \mathbf{H}^{1/2}(\widehat \theta_n - \theta_*)\right\rangle + \frac{1}{2} \Vert\widehat\theta_n - \theta_*\Vert_{\mathbf{H}}^2 \\
&amp;\ge -\Vert \nabla L_n(\theta_*) \Vert_{\mathbf{H}^{-1}} \Vert\widehat \theta_n - \theta_* \Vert_{\mathbf{H}} + \frac{1}{2} \Vert\widehat\theta_n - \theta_*\Vert_{\mathbf{H}}^2. 
\end{align}
where the transition to the second line uses that $\mathbf{H}_n$ converges to $\mathbf{H}$, and the last transition is by Cauchy-Schwarz.
As a result, we arrive at
\[
\frac{1}{2} \Vert\widehat\theta_n - \theta_*\Vert_{\mathbf{H}}^2 \le 2 \Vert\nabla L_n(\theta_*)\Vert_{\mathbf{H}^{-1}}^2.
\]</p>
  </li>
  <li>
    <p>Once the localization is achieved, one can similarly control the excess risk $L(\widehat\theta_n) - L(\theta_*)$ through its second-order Taylor approximation $\frac{1}{2} \Vert\widehat\theta_n - \theta_*\Vert_{\mathbf{H}}^2$. 
For this, one shows that the Hessian at $\theta$,
\[
\mathbf{H}(\theta) := \nabla^2 L(\theta),
\] 
remains nearly constant in the <strong>Dikin ellipsoid</strong>
\[
\Theta_{r}(\theta_*) = \left\{ \theta \in \mathbb{R}^d: \Vert\theta - \theta_*\Vert_{\mathbf{H}} \le r \right\}
\]
with a constant radius $r = O(1)$, where by ``near-constant’’ we mean that 
\[
c \mathbf{H}(\theta_*) \preccurlyeq \mathbf{H}(\theta) \preccurlyeq C\mathbf{H}(\theta_*)
\] 
in the positive-semidefinite sense for some constants $0 &lt; c \le 1$ and $C \ge 1$, concisely written as
\[
\mathbf{H}(\theta) \asymp \mathbf{H}(\theta_*).
\]
This classical fact, whose proof we omit here, can be obtained from the relation between the second and third moments of the <em>calibrated design</em> – the vector $\tilde X$ satisfying $\mathbb{E}[\tilde X \tilde X^{\top}] = \mathbf{H}$:
\[
\tilde X(\theta_*) := [\ell_{\eta}''(Y,X^\top \theta_*)]^{1/2} X.
\]</p>
  </li>
</ol>

<h1 id="finite-sample-setup-the-challenge"><strong>Finite-sample setup: the challenge</strong></h1>
<p>When we want to prove a non-asymptotic analogue of \eqref{eq:crb-prob} in the finite-sample setup, the first step of the asymptotic ‘‘recipe’’ remains more or less the same: one can simply use Hoeffding’s inequality instead of the central limit theorem to control $\Vert \nabla L_n(\theta_*) \Vert_{\mathbf{H}^{-1}}^2$ under subgaussian assumptions on the loss gradients. 
Also, the third step relies to a non-statistical argument since the sample size does not figure in it at all, so there are no changes here as well.
Thus, the challenge is in the second (localization) step: now we must prove that $L_n(\widehat\theta_n) - L_n(\theta_*)$ is close to its second-order Taylor approximation centered at $\theta_*$, without taking the limit. As we will see a bit later, this could be reduced to showing that $\mathbf{H}_n(\theta)$ is near-constant, with high probability, uniformly over the Dikin ellipsoid $\Theta_{c}(\theta_*)$ with constant radius. Since the same property is known to be  for the non-random Hessian $\mathbf{H}(\theta)$, our task boils down to bounding the uniform deviations of $\mathbf{H}_n(\theta)$ from $\mathbf{H}(\theta)$ on $\Theta_{c}(\theta_*)$. Generally, this task is rather complicated, and requires the advanced theory of empirical processes together with some global assumptions on the whole domain of $\theta$, see, e.g., <a href="https://projecteuclid.org/euclid.aos/1360332187#info">[2]</a>.
However, in some cases it can be made simpler through the delicate use of <strong>self-concordance</strong>. 
This concept was introduced in <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791?mobileUi=0">[3]</a> in the context of interior-point methods, and brought to the attention of the statistical learning community in <a href="https://projecteuclid.org/euclid.ejs/1271941980">[4]</a>. 
Our next goal is to understand the role of this concept in the finite-sample analysis.</p>

<h1 id="simple-case-least-squares"><strong>Simple case: least squares</strong></h1>
<p>In the simplest case of (linear) least-squares, that is, when
\[
\ell(Y,X^\top \theta) = \frac{1}{2} (Y - X^\top\theta)^2,
\]
the analysis is rather straightforward since $L_n(\theta)$ and $L(\theta)$ are quadratic forms. All that is needed is to apply a concentration inequality to $\Vert\nabla L_n(\theta_*)\Vert_{\mathbf{H}}$, and make sure that $\mathbf{H}_n \asymp \mathbf{H}$ to guarantee that the first transition in the chain \eqref{eq:localization-chain} remains valid.
Specifically, recalling the definition of calibrated design $\tilde X(\theta)$, we see that In least-squares it coincides with $X$ at any point $\theta$, hence,
\[
\mathbf{H} \equiv \mathbb{E}[X X^{\top}], \quad \mathbf{H}_n \equiv \frac{1}{n}\sum_{i=1}^n X_i X_i^{\top}.
\]
Thus, the analysis is thus reduced to controlling the deviations of a <em>single</em> sample covariance matrix – that of $X$ – from its expectation. This can be done using the well-known result from <a href="https://arxiv.org/abs/1011.3027">[5]</a>: assuming that the decorrelated design $\mathbf{H}^{-1/2} X$ is $K$-<em>subgaussian</em>, that is, for any direction $u \in \mathbb{R}^d$ it holds
\[
\mathbb{E}\left[\exp \left\langle u, \mathbf{H}^{-1/2} X \right\rangle \right] \le \exp(K^2\Vert u\Vert_2^2/2),
\]
we have $\mathbf{H}_n \asymp \mathbf{H}$
with probability at least $1-\delta$ as soon as 
\[
\boxed{
n \gtrsim K^4  (d + \log(1/\delta)),
}
\]
where $a \gtrsim b$ is a shorthand for $b = O(a)$.
When combined together, this gives the $O(d/n)$ rate in the regime $n = \Omega(d)$. Next we will show how to extend this result, first obtained in <a href="http://proceedings.mlr.press/v23/hsu12/hsu12.pdf">[6]</a>, beyond the case of least squares.</p>

<h1 id="key-observation-localization-lemma"><strong>Key observation: Localization Lemma</strong></h1>

<p>Before we embark to self-concordance, I will demonstrate that the localization of $\widehat\theta_n$ in the Dikin ellipsoid $\Theta_r(\theta_*)$ of some radius $r$ is guaranteed if we show the uniform approximation bound 
\[
\mathbf{H}_n(\theta) \asymp \mathbf{H}_n(\theta_*)
\]
on $\Theta_r(\theta_*)$. In what follows, we assume that the loss is convex. 
Also, we assume that the (decorrelated) calibrated design $\mathbf{H}^{-1/2} \tilde X(\theta_*)$, is $K_2$-subgaussian, and for some $\delta \in (0,1)$ it holds
\[
n \gtrsim K_2^4 (d + \log(1/\delta)),
\] 
so that we can apply the result of <a href="https://arxiv.org/abs/1011.3027">[5]</a>, and with probability $1-\delta$ identify the empirical and true Hessians at a <em>single point</em> $\theta_*$: $\mathbf{H}_n \asymp \mathbf{H}$ (recall that we use $\mathbf{H}$ and $\mathbf{H}_n$, without parentheses, as shorthands for $\mathbf{H}(\theta_*)$ and $\mathbf{H}_n(\theta_*)$).
We then have an auxiliary result called the Localization Lemma.</p>

<blockquote>
  <p><strong>Localization lemma.</strong>
Suppose that $n \gtrsim K_2^4 (d + \log(1/\delta))$, and for some $r \ge 0$ it holds
\[
\mathbf{H}_n(\theta) \asymp \mathbf{H}_n(\theta_*), \, \forall \theta \in \Theta_{r}(\theta_*).
\] 
Then, for any $r_0 \le r$, the following holds: whenever
\[
\Vert\nabla L_n(\theta_*) \Vert_{\mathbf{H}^{-1}}^2 \lesssim r_0^2,
\]
we have that $\widehat\theta_n$ belongs to $\Theta_{r_0}(\theta_*)$, and moreover,
\[
L(\widehat \theta_n) - L(\theta_*) \lesssim \Vert\widehat \theta_n - \theta_*\Vert_{\mathbf{H}}^2 \lesssim \Vert\nabla L_n(\theta_*) \Vert_{\mathbf{H}^{-1}}^2.
\]</p>
</blockquote>

<p><strong>Proof sketch.</strong> By definition, $L_n(\widehat\theta_n) \le L_n(\theta_*)$. Assume that $\widehat\theta_n$ is not in $\Theta_{r_0}(\theta_*)$, and choose the point $\bar \theta_n$ on the segment $[\theta_*,\widehat\theta_n]$ such that $\bar \theta_n$ is precisely on the boundary of $\Theta_{r_0}(\theta_*)$, so that 
\[
\Vert\bar \theta_n - \theta_*\Vert_{\mathbf{H}} = r_0.
\]
Note that by convexity of the level sets of $L_n(\theta)$, we still have $L_n(\bar\theta_n) \le L_n(\theta_*).$
On the other hand, by the intermediate value theorem, for some $\theta’_n$ belonging to the segment $[\theta_*, \bar\theta_n]$, and hence to $\Theta_{r_0}(\theta_*)$, it holds
\begin{align}
0 \ge L_n(\bar\theta_n) - L_n(\theta_*) 
&amp;= \left\langle \nabla L_n(\theta_*), \bar\theta_n - \theta_* \right\rangle + \frac{1}{2} \Vert \bar\theta_n - \theta_*\Vert_{\mathbf{H}_n(\theta’_n)}^2 \\
&amp;\approx \left\langle \nabla L_n(\theta_*), \bar\theta_n - \theta_* \right\rangle + \frac{1}{2} \Vert \bar\theta_n - \theta_* \Vert_{\mathbf{H}_n}^2 \\
&amp;\approx \left\langle \nabla L_n(\theta_*), \bar\theta_n - \theta_* \right\rangle + \frac{1}{2} \Vert \bar\theta_n - \theta_*\Vert_{\mathbf{H}}^2 \\
&amp;\ge - \Vert \nabla L_n(\theta_*) \Vert_{\mathbf{H}^{-1}} \Vert\bar\theta_n - \theta_* \Vert_{\mathbf{H}} + \frac{1}{2} \Vert \bar\theta_n - \theta_*\Vert_{\mathbf{H}}^2 \\
&amp;= - r_0 \Vert \nabla L_n(\theta_*) \Vert_{\mathbf{H}^{-1}} + \frac{r_0^2}{2},
\end{align}
where we use $a \approx b$ as a shorthand for saying that $a$ and $b$ are within a multiplicative constant factor from each other, and used that~$\mathbf{H}_n(\theta’_n) \asymp \mathbf{H}_n(\theta_*)$ in the second line.
Rearranging the terms, we arrive at a contradiction, so in fact $\widehat\theta_n$ must belong to $\Theta_{r_0}(\theta_*)$. 
This proves the first claim of the lemma. 
Now that we know that $\widehat\theta_n \in \Theta_{r_0}(\theta_*)$, we can also prove that 
\[
\Vert\widehat \theta_n - \theta_*\Vert_{\mathbf{H}}^2 \lesssim \Vert\nabla L_n(\theta_*) \Vert_{\mathbf{H}^{-1}}^2
\]
by replacing $\bar \theta_n$ with $\widehat\theta_n$ in the above chain of inequalities. Finally, 
\[
L(\widehat \theta_n) - L(\theta_*) \lesssim \Vert\widehat \theta_n - \theta_*\Vert_{\mathbf{H}}^2
\]
also follows from the intermediate value theorem. using that $\widehat \theta_n$ belongs to the ellipsoid $\Theta_r(\theta_*)$ in which $\mathbf{H}_n(\theta) \asymp \mathbf{H}_n(\theta_*)$, and applying the sample covariance matrix concentration result to $\mathbf{H}_n(\theta_*)$. $\blacksquare$</p>

<h1 id="why-the-constant-radius"><strong>Why the constant radius?</strong></h1>

<p>Recall that the gradient of the empirical risk is the average of i.i.d. random vectors,
\[
\nabla L_n(\theta_*) = \frac{1}{n} \sum_{i = 1}^n \nabla_{\theta} \ell(Y_i,X_i^\top \theta_*),
\]
and each $\nabla_{\theta} \ell(Y_i,X_i^\top \theta)$ has covariance $\mathbf{G}$. Assuming that decorrelated gradients $\mathbf{G}^{-1/2}\nabla_{\theta} \ell(Y_i,X_i^\top \theta_*)$ are $K_1$-subgaussian, Bernstein inequality implies, with probability $\ge 1-\delta$,
\[
\Vert\nabla L_n(\theta_*) \Vert_{\mathbf{H}^{-1}}^2 \lesssim \frac{K_1^2 d_{eff} \log(1/\delta)}{n}.
\]
Hence, the localization lemma implies that if we can guarantee that $\mathbf{H}_n(\theta)$ is near-constant over the Dikin ellipsoid of radius $r$, the sample size sufficient to guarantee a finite-sample analogue of \eqref{eq:crb-prob} is
\[
\boxed{
n \gtrsim \max \left\{ K_2^4 (d+ \log(1/\delta)),  \; \color{red}{r^2} K^2 K_1^2 {\color{blue}{d_{eff}}} \log(1/\delta) \right\}.
}
\] 
The first bound guarantees reliable estimation of the risk curvature at the optimum, and is the same as in linear regression, so we have reasons to suggest that it is unavoidable. 
On the other hand, the second bound is related to the fact that the loss is not quadratic, and dominates the first one, assuming $d_{eff} = O(d)$, unless $r$ – the radius of the Dikin ellipsoid in which $\mathbf{H}_n(\theta) \asymp \mathbf{H}_n(\theta_*)$ with high probability – is <em>constant</em>. 
In the next post, we will see how self-concordance leads to Hessian approximation bounds of this type with $r = O(\sqrt{d})$ which results in the $O(d \cdot d_{eff})$ sample size, and how this can be improved to $r = O(1)$ and 
$
n = O(\max(d,d_{eff}))
$
with a more subtle argument.</p>

<h1 id="references"><strong>References</strong></h1>

<p><a href="https://projecteuclid.org/euclid.aos/1015957395#info">[1] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection.</a> 
<em>Ann. Statist.</em>, 28:5(2000), 1302-1338.</p>

<p><a href="https://projecteuclid.org/euclid.aos/1360332187#info">[2] V. Spokoiny. Parametric estimation. Finite sample theory.</a> 
<em>Ann. Statist.</em>, 40:6(2012), 2877-2909.</p>

<p><a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970791?mobileUi=0">[3] A. Nemirovski and Yu. Nesterov. Interior-point polynomial algorithms in convex programming.</a> 
<em>Society for Industrial and Applied Mathematics, Philadelphia, 1994.</em></p>

<p><a href="https://projecteuclid.org/euclid.ejs/1271941980">[4] F. Bach. Self-concordant analysis for logistic regression.</a> 
<em>Electron. J. Stat., 4(2010), 384-414.</em></p>

<p><a href="https://arxiv.org/abs/1011.3027">[5] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices.</a> 
<em>Compressed Sensing: Theory and Applications, 210–268</em>. Cambridge University Press, 2012.</p>

<p><a href="http://proceedings.mlr.press/v23/hsu12/hsu12.pdf">[6] D. Hsu, S. Kakade, and T. Zhang. Random design analysis of ridge regression.</a> <em>COLT, 2012.</em></p>

  </div><a class="u-url" href="/jekyll/update/2018/11/12/self-concordance-part-1.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Personal blog of Dmitrii Ostrovskii</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Personal blog of Dmitrii Ostrovskii</li><li><a class="u-email" href="mailto:dmitrii.ostrovskii@inria.fr">dmitrii.ostrovskii@inria.fr</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/ostrodmit"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">ostrodmit</span></a></li><li><a href="https://www.twitter.com/"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username"></span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
